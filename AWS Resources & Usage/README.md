# AWS Resources:

## AWS CloudWatch: Overview and Usage in Data Engineering Projects

### What is AWS CloudWatch?
AWS **CloudWatch** is a **monitoring, logging, and observability** service that helps you track the performance and operational health of AWS resources and applications. It enables you to:
- Collect **metrics** and **logs** from AWS services and custom applications.
- Set up **alarms** for performance monitoring and automated responses.
- Analyze, visualize, and debug **system performance** and **errors**.

## Key Features of AWS CloudWatch
1. **Metrics Collection and Monitoring**
   - Collects real-time metrics like CPU utilization, disk I/O, memory usage, and network traffic.
   - Monitors AWS resources such as EC2, RDS, Lambda, and more.

2. **Log Management with CloudWatch Logs**
   - Stores and retrieves **application logs** for troubleshooting and debugging.
   - Enables filtering and searching for **specific log events**.

3. **Alerts and Notifications**
   - CloudWatch **Alarms** notify you when a metric exceeds a defined threshold.
   - Can trigger automated actions (e.g., scaling, notifications via SNS).

4. **Dashboards for Visualization**
   - Provides customizable **dashboards** to visualize metrics and logs in one place.
   - Useful for monitoring trends over time.

5. **Event Monitoring and Automation**
   - CloudWatch **Events** and **EventBridge** automate responses to changes in your environment (e.g., starting a Lambda function when new data arrives in S3).

6. **Anomaly Detection**
   - Uses **Machine Learning** to detect performance anomalies in metrics.

7. **Integration with Other AWS Services**
   - Works seamlessly with AWS Lambda, ECS, DynamoDB, S3, and others to provide a holistic monitoring solution.

---

## Using AWS CloudWatch in Data Engineering Projects

### 1. **Monitoring Data Pipelines**
CloudWatch can monitor critical components of a data pipeline, such as:
- **ETL Jobs** (AWS Glue, Apache Spark on EMR)
  - Track metrics like job completion times, memory usage, and failed jobs.
- **Data Storage** (S3, DynamoDB, RDS)
  - Monitor the size, throughput, and I/O performance of storage systems.
- **Message Queues** (Amazon SQS, Kafka)
  - Watch for queue lengths, message delays, and processing rates.

### 2. **Log Management**
- **CloudWatch Logs** can store and analyze logs generated by:
  - ETL tools (e.g., AWS Glue logs, Spark executor logs).
  - Data pipeline errors and exceptions.
  - Application logs from custom-built data integration tools.
- Example: Track errors in data ingestion jobs from S3 to Redshift.

### 3. **Setting Alarms for Critical Metrics**
- Set up **CloudWatch Alarms** for key metrics:
  - Notify the team if an ETL job fails or takes longer than expected.
  - Trigger a Lambda function to retry a failed step in the pipeline.
- Example:
  - Alarm: Trigger an alert if the S3 bucket size exceeds a threshold.
  - Automation: Archive old data using Lambda when the alarm is triggered.

### 4. **Automation with CloudWatch Events**
- Use **CloudWatch Events** to automate actions:
  - Trigger AWS Glue ETL jobs when new data is added to an S3 bucket.
  - Start a data validation Lambda function after pipeline completion.
- Example: Event-driven architecture for real-time data pipelines.

### 5. **Performance Optimization**
- Monitor resource utilization in real-time to optimize performance:
  - Scale EC2 instances dynamically based on CPU or memory usage.
  - Analyze trends to allocate resources effectively.

### 6. **Error Debugging**
- Use **CloudWatch Logs Insights** to query logs:
  - Identify failed steps in a data pipeline.
  - Search for specific error messages or patterns in logs.

---

## Example: Setting Up CloudWatch in a Data Engineering Project

### Step 1: Enable Monitoring for AWS Resources
1. Enable **detailed monitoring** for EC2, RDS, and other AWS services.
2. Configure **AWS Glue jobs** to log to **CloudWatch Logs**.

### Step 2: Create CloudWatch Alarms
- Example: Create an alarm for S3 bucket size.
```python
import boto3

cloudwatch = boto3.client('cloudwatch')
response = cloudwatch.put_metric_alarm(
    AlarmName='S3BucketSizeAlarm',
    MetricName='BucketSizeBytes',
    Namespace='AWS/S3',
    Statistic='Average',
    Period=300,
    EvaluationPeriods=1,
    Threshold=5000000000,  # 5GB
    ComparisonOperator='GreaterThanThreshold',
    AlarmActions=['arn:aws:sns:us-east-1:123456789012:MySNSTopic']
)

```

### Step 3: Automate Responses with CloudWatch Events
- **Example:** Trigger AWS Glue ETL job when new data is added to an S3 bucket.

```json
{
  "source": ["aws.s3"],
  "detail-type": ["Object Created"],
  "resources": ["arn:aws:s3:::my-data-bucket"],
  "detail": {
    "bucket-name": ["my-data-bucket"]
  }
}

```

### Step 4: Visualize Metrics with Dashboards
- Create a CloudWatch dashboard to monitor:
   - S3 bucket size trends.
   - ETL job completion rates.
   - Data pipeline throughput.

---

# AWS Lambda: A Complete Overview

## What is AWS Lambda?
AWS Lambda is a **serverless compute service** provided by AWS that allows you to run code **without provisioning or managing servers**. It automatically scales and only charges for the compute time consumed.

### Key Features of AWS Lambda:
- **Serverless**: No need to manage infrastructure.
- **Event-driven**: Executes code in response to triggers (e.g., S3 file upload, API Gateway request).
- **Auto-scaling**: Handles multiple concurrent executions.
- **Cost-effective**: Only pay for execution time.
- **Supports Multiple Languages**: Python, Node.js, Java, C#, Go, etc.
- **Integrated with AWS Services**: Works with S3, DynamoDB, Kinesis, SNS, SQS, etc.

---

## How Data Engineers Use AWS Lambda

Data Engineers often use AWS Lambda in **ETL pipelines**, **data streaming**, **event-driven workflows**, and **automation tasks**. Below are some key use cases:

### 1. **Data Ingestion and Preprocessing**
   - AWS Lambda can trigger when new data arrives in **Amazon S3**, **DynamoDB**, or **Kinesis Streams**.
   - It can process, transform, and forward data to another storage or database.

   **Example Use Case**:
   - A new CSV file is uploaded to S3.
   - AWS Lambda extracts and processes the data.
   - The transformed data is stored in **Amazon Redshift** or **DynamoDB**.

### 2. **Automating ETL Pipelines**
   - AWS Lambda is used to orchestrate and automate Extract, Transform, and Load (ETL) tasks.
   - It can trigger AWS Glue or process small datasets directly.

   **Example Flow**:
   - Extract: Read data from an S3 bucket or API.
   - Transform: Process data (convert formats, clean data).
   - Load: Store in Redshift, DynamoDB, or another data warehouse.

### 3. **Real-time Data Processing with AWS Kinesis**
   - AWS Lambda can process real-time streaming data from **Amazon Kinesis** or **Kafka**.
   - It helps with tasks like filtering logs, detecting anomalies, and enriching data streams.

   **Example Use Case**:
   - Clickstream data is sent to AWS Kinesis.
   - AWS Lambda processes user interactions and pushes insights to an S3 data lake.

### 4. **Data Validation and Cleaning**
   - AWS Lambda can validate incoming data and apply data quality rules.
   - It can reject or log invalid data for further inspection.

   **Example Use Case**:
   - A JSON payload is sent to an API Gateway.
   - AWS Lambda validates the data format and stores valid records in a database.

### 5. **Trigger-Based Workflows**
   - Lambda can integrate with AWS Step Functions for orchestrating complex workflows.
   - It automates data pipelines with error handling and retries.

   **Example Use Case**:
   - A Lambda function extracts data from an API.
   - If extraction is successful, another Lambda function processes and stores the data.
   - If it fails, an SNS notification is sent to a monitoring team.

### 6. **Scheduled Jobs and Automation**
   - AWS Lambda can run periodic tasks using **Amazon EventBridge (CloudWatch)**.
   - It is used for:
     - Automating database backups.
     - Cleaning up old files from S3.
     - Running scheduled data quality checks.

---

## AWS Lambda Example Code (Python)
Below is an **example Lambda function** that gets triggered when a new file is uploaded to an **S3 bucket** and logs the file name.

```python
import json
import boto3

def lambda_handler(event, context):
    s3_client = boto3.client('s3')
    
    # Get bucket and object details from event
    for record in event['Records']:
        bucket_name = record['s3']['bucket']['name']
        file_key = record['s3']['object']['key']
        
        print(f"New file uploaded: {file_key} in bucket {bucket_name}")
    
    return {
        'statusCode': 200,
        'body': json.dumps('File processed successfully!')
    }

```

### Explanation:
- Triggered by S3 events (when a file is uploaded).
- Retrieves the bucket name and file name.
- Prints/logs the filename for further processing.

### Benefits of Using AWS Lambda for Data Engineering
- **Reduces Infrastructure Overhead:** No need to manage servers.
- **Cost-efficient:** Only pay for execution time.
- **Event-driven Processing:** Easily integrates with AWS services.
- **Scalable:** Automatically handles concurrent requests.
- **Faster Development:** Focus on logic, not infrastructure.

### When Not to Use AWS Lambda?
- **Long-running processes (>15 min timeout)** → Use AWS Glue or EC2.
- **Large data transformations** → Use EMR, Glue, or Fargate.
- **High-frequency execution (>1000 requests per second)** → May become costly.
